"""
config.py - Configuraci√≥n OPTIMIZADA para Q-Learning con Q-Table en Frozen Lake

Esta versi√≥n optimizada mejora la convergencia y aumenta la frecuencia de rutas √≥ptimas
basada en el an√°lisis de los resultados de entrenamiento anteriores.
"""

import numpy as np

# ================================
# CONFIGURACI√ìN DEL ENTORNO
# ================================
class EnvConfig:
    """Configuraci√≥n del entorno Frozen Lake"""
    
    MAP_SIZE = "4x4"
    IS_SLIPPERY = True
    STATE_SIZE = 16  # Estados 0-15
    ACTION_SIZE = 4  # Izquierda, Abajo, Derecha, Arriba
    MAX_STEPS = 100
    RENDER_MODE = None

# ================================
# HIPERPAR√ÅMETROS DE Q-LEARNING OPTIMIZADOS
# ================================
class QLearningConfig:
    """Configuraci√≥n optimizada de Q-Learning con tabla Q"""
    
    # Par√°metros de aprendizaje mejorados
    LEARNING_RATE = 0.15       # Alpha - incrementado para aprendizaje m√°s r√°pido
    DISCOUNT_FACTOR = 0.95     # Gamma - reducido para valorar m√°s recompensas inmediatas
    
    # Exploraci√≥n optimizada (Epsilon-Greedy)
    EPSILON_START = 0.9        # Menos exploraci√≥n inicial
    EPSILON_END = 0.05         # M√°s exploraci√≥n final para encontrar rutas √≥ptimas
    EPSILON_DECAY = 0.9992     # Decay m√°s lento para mejor exploraci√≥n
    
    # Configuraci√≥n de convergencia
    MIN_LEARNING_RATE = 0.02   # Learning rate m√≠nimo m√°s alto
    LR_DECAY = 0.9998          # Decay m√°s lento del learning rate

# ================================
# CONFIGURACI√ìN DE ENTRENAMIENTO OPTIMIZADA
# ================================
class TrainingConfig:
    """Configuraci√≥n optimizada del entrenamiento"""
    
    NUM_EPISODES = 12000       # M√°s episodios para convergencia completa
    PRINT_EVERY = 500          # Reportar cada 500 episodios
    SAVE_EVERY = 2500          # Guardar cada 2500 episodios
    
    # Criterios de convergencia mejorados
    SOLVE_SCORE = 0.75         # Objetivo m√°s realista para entorno stochastic
    SOLVE_WINDOW = 200         # Ventana m√°s grande para evaluaci√≥n
    
    # Archivos
    MODEL_PATH = "frozen_lake_qtable_optimized.npy"
    RESULTS_PATH = "qtable_results_optimized.png"

# ================================
# CONFIGURACI√ìN DE EVALUACI√ìN
# ================================
class EvalConfig:
    """Configuraci√≥n para evaluaci√≥n"""
    
    EVAL_EPISODES = 2000       # M√°s episodios para evaluaci√≥n estad√≠sticamente significativa
    DEMO_EPISODES = 8          # M√°s demos para mostrar
    
    # Objetivos optimizados
    TARGET_SUCCESS_RATE = 0.7  # Objetivo m√°s realista
    TARGET_OPTIMAL_RATE = 0.1  # 10% de rutas √≥ptimas (m√°s ambicioso)
    OPTIMAL_STEPS = 6

# ================================
# CONFIGURACI√ìN DE RECOMPENSAS OPTIMIZADA
# ================================
class RewardConfig:
    """Sistema de recompensas optimizado para maximizar rutas √≥ptimas"""
    
    # Recompensas principales amplificadas
    SUCCESS_REWARD = 1000.0         # Incrementado significativamente
    OPTIMAL_BONUS = 500.0           # Bonificaci√≥n masiva por ruta √≥ptima
    THEORETICAL_BONUS = 200.0       # Bonificaci√≥n adicional por ruta te√≥rica exacta
    
    # Recompensas de navegaci√≥n refinadas
    PROGRESS_REWARD = 5.0           # Incrementado para motivar progreso
    REGRESS_PENALTY = -2.0          # Penalizaci√≥n m√°s fuerte por retroceder
    HOLE_PENALTY = -50.0            # Penalizaci√≥n moderada por agujero
    STEP_PENALTY = -0.5             # Penalizaci√≥n por paso m√°s significativa
    
    # Exploraci√≥n dirigida mejorada
    EXPLORATION_BONUS = 2.0         # Bonificaci√≥n aumentada por exploraci√≥n
    OPTIMAL_PATH_BONUS = 3.0        # Nueva: bonificaci√≥n por estar en ruta √≥ptima
    
    # Nuevas recompensas para incentivar eficiencia
    EFFICIENCY_BONUS_BASE = 50.0    # Bonificaci√≥n base por eficiencia
    EFFICIENCY_THRESHOLD = 10       # Umbral para bonificaci√≥n de eficiencia

# ================================
# RUTAS √ìPTIMAS CONOCIDAS (sin cambios)
# ================================
class OptimalPaths:
    """Rutas √≥ptimas conocidas y funciones de an√°lisis"""
    
    PATHS_4X4 = [
        [0, 1, 2, 6, 10, 14, 15],    # Ruta Norte-Este
        [0, 4, 8, 9, 10, 14, 15],    # Ruta Sur-Este (variante 1)
        [0, 4, 8, 9, 13, 14, 15],    # Ruta Sur-Este (variante 2)
    ]
    
    @classmethod
    def get_optimal_action(cls, state):
        """
        Obtener acci√≥n √≥ptima para un estado dado
        
        Args:
            state (int): Estado actual
            
        Returns:
            int or None: Acci√≥n √≥ptima o None si no est√° en ruta √≥ptima
        """
        for path in cls.PATHS_4X4:
            if state in path:
                current_idx = path.index(state)
                if current_idx < len(path) - 1:
                    next_state = path[current_idx + 1]
                    return cls._calculate_action(state, next_state)
        return None
    
    @classmethod
    def _calculate_action(cls, from_state, to_state):
        """Calcular acci√≥n necesaria para ir de un estado a otro"""
        from_row, from_col = divmod(from_state, 4)
        to_row, to_col = divmod(to_state, 4)
        
        if to_row > from_row:
            return 1  # Abajo
        elif to_row < from_row:
            return 3  # Arriba
        elif to_col > from_col:
            return 2  # Derecha
        elif to_col < from_col:
            return 0  # Izquierda
        
        return None
    
    @classmethod
    def is_optimal_path(cls, path):
        """Verificar si un camino es una ruta √≥ptima te√≥rica"""
        for i, theoretical_path in enumerate(cls.PATHS_4X4):
            if path == theoretical_path:
                return i
        return None
    
    @classmethod
    def is_on_optimal_path(cls, state):
        """Verificar si un estado est√° en alguna ruta √≥ptima"""
        for path in cls.PATHS_4X4:
            if state in path:
                return True
        return False
    
    @classmethod
    def get_path_progress(cls, state):
        """Obtener progreso en rutas √≥ptimas (nuevo)"""
        max_progress = 0
        for path in cls.PATHS_4X4:
            if state in path:
                progress = path.index(state) / (len(path) - 1)
                max_progress = max(max_progress, progress)
        return max_progress

# ================================
# CONFIGURACI√ìN DE LOGGING (sin cambios)
# ================================
class LogConfig:
    """Configuraci√≥n de logging y visualizaci√≥n"""
    
    COLORS = {
        'SUCCESS': '\033[92m',
        'WARNING': '\033[93m',
        'ERROR': '\033[91m',
        'INFO': '\033[94m',
        'BOLD': '\033[1m',
        'END': '\033[0m'
    }
    
    EMOJIS = {
        'SUCCESS': '‚úÖ',
        'OPTIMAL': 'üéØ',
        'WARNING': '‚ö†Ô∏è',
        'ERROR': '‚ùå',
        'INFO': '‚ÑπÔ∏è',
        'TRAIN': 'üöÄ',
        'EVAL': 'üî¨',
        'SAVE': 'üíæ',
        'QTABLE': 'üìä',
        'THEORETICAL': 'üèÜ',
        'EFFICIENCY': '‚ö°',
        'IMPROVEMENT': 'üìà'
    }

# ================================
# FUNCIONES DE UTILIDAD MEJORADAS
# ================================
def get_manhattan_distance(state, goal_state=15):
    """Calcular distancia Manhattan entre dos estados"""
    state_row, state_col = divmod(state, 4)
    goal_row, goal_col = divmod(goal_state, 4)
    return abs(state_row - goal_row) + abs(state_col - goal_col)

def calculate_efficiency_bonus(steps):
    """Calcular bonificaci√≥n por eficiencia (nuevo)"""
    if steps <= EvalConfig.OPTIMAL_STEPS:
        return RewardConfig.EFFICIENCY_BONUS_BASE * 2  # Doble bonificaci√≥n para rutas √≥ptimas
    elif steps <= RewardConfig.EFFICIENCY_THRESHOLD:
        # Bonificaci√≥n graduada para rutas eficientes
        efficiency_ratio = (RewardConfig.EFFICIENCY_THRESHOLD - steps) / RewardConfig.EFFICIENCY_THRESHOLD
        return RewardConfig.EFFICIENCY_BONUS_BASE * efficiency_ratio
    else:
        return 0

def print_config():
    """Imprimir configuraci√≥n optimizada actual"""
    print(f"{LogConfig.COLORS['BOLD']}=== CONFIGURACI√ìN Q-LEARNING OPTIMIZADA FROZEN LAKE ==={LogConfig.COLORS['END']}")
    print(f"Algoritmo: Q-Learning con Tabla Q (OPTIMIZADO)")
    print(f"Estados: {EnvConfig.STATE_SIZE}")
    print(f"Acciones: {EnvConfig.ACTION_SIZE}")
    print(f"Episodios: {TrainingConfig.NUM_EPISODES}")
    print(f"Learning Rate: {QLearningConfig.LEARNING_RATE} ‚Üí {QLearningConfig.MIN_LEARNING_RATE}")
    print(f"Epsilon: {QLearningConfig.EPSILON_START} ‚Üí {QLearningConfig.EPSILON_END}")
    print(f"Gamma: {QLearningConfig.DISCOUNT_FACTOR}")
    print(f"Recompensas optimizadas: ‚úÖ")
    print(f"Exploraci√≥n dirigida mejorada: ‚úÖ")
    print(f"Bonificaciones de eficiencia: ‚úÖ")
    print("-" * 60)

def validate_config():
    """Validar configuraci√≥n optimizada"""
    assert EnvConfig.STATE_SIZE == 16, "STATE_SIZE debe ser 16 para mapa 4x4"
    assert EnvConfig.ACTION_SIZE == 4, "ACTION_SIZE debe ser 4"
    assert 0 < QLearningConfig.LEARNING_RATE <= 1, "Learning rate debe estar entre 0 y 1"
    assert 0 < QLearningConfig.DISCOUNT_FACTOR <= 1, "Gamma debe estar entre 0 y 1"
    assert 0 <= QLearningConfig.EPSILON_END < QLearningConfig.EPSILON_START <= 1, "Epsilon inv√°lido"
    
    # Validar rutas √≥ptimas
    for i, path in enumerate(OptimalPaths.PATHS_4X4):
        assert len(path) == 7, f"Ruta {i+1} debe tener 7 estados"
        assert path[0] == 0 and path[-1] == 15, f"Ruta {i+1} debe ir de 0 a 15"
        assert all(0 <= state <= 15 for state in path), f"Ruta {i+1} contiene estados inv√°lidos"
    
    print(f"{LogConfig.EMOJIS['SUCCESS']} Configuraci√≥n optimizada validada correctamente")

def get_optimization_summary():
    """Obtener resumen de optimizaciones implementadas"""
    optimizations = [
        "üéØ Learning rate incrementado para convergencia m√°s r√°pida",
        "üéØ Gamma reducido para valorar recompensas inmediatas",
        "üéØ Epsilon decay m√°s lento para mejor exploraci√≥n",
        "üéØ Recompensas amplificadas para rutas √≥ptimas",
        "üéØ Bonificaciones de eficiencia graduadas",
        "üéØ Penalizaciones balanceadas para guiar aprendizaje",
        "üéØ M√°s episodios para convergencia completa",
        "üéØ Evaluaci√≥n estad√≠sticamente m√°s robusta"
    ]
    return optimizations

if __name__ == "__main__":
    print_config()
    validate_config()
    
    print(f"\n{LogConfig.EMOJIS['OPTIMAL']} Rutas √≥ptimas objetivo:")
    for i, path in enumerate(OptimalPaths.PATHS_4X4, 1):
        path_str = " ‚Üí ".join(map(str, path))
        print(f"  {i}. {path_str}")
    
    print(f"\n{LogConfig.EMOJIS['IMPROVEMENT']} Optimizaciones implementadas:")
    for optimization in get_optimization_summary():
        print(f"  {optimization}")
    
    print(f"\n{LogConfig.EMOJIS['INFO']} Ventajas de la configuraci√≥n optimizada:")
    print("  ‚Ä¢ Convergencia m√°s r√°pida y estable")
    print("  ‚Ä¢ Mayor frecuencia de rutas √≥ptimas")
    print("  ‚Ä¢ Exploraci√≥n m√°s inteligente")
    print("  ‚Ä¢ Recompensas balanceadas para eficiencia")
    print("  ‚Ä¢ Evaluaci√≥n estad√≠sticamente robusta")