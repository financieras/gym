"""
config.py - ConfiguraciÃ³n e HiperparÃ¡metros para DQN en Frozen Lake

Este archivo contiene todas las configuraciones y hiperparÃ¡metros necesarios
para entrenar el agente DQN mejorado en el entorno Frozen Lake.
"""

import torch

# ================================
# CONFIGURACIÃ“N DEL DISPOSITIVO
# ================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================================
# CONFIGURACIÃ“N DEL ENTORNO
# ================================
class EnvConfig:
    """ConfiguraciÃ³n del entorno Frozen Lake"""
    
    # ConfiguraciÃ³n del mapa
    MAP_SIZE = "4x4"  # "4x4" o "8x8"
    IS_SLIPPERY = True  # True para entorno estocÃ¡stico, False para determinÃ­stico
    
    # Espacios de estado y acciÃ³n
    STATE_SIZE = 16  # Para mapa 4x4
    ACTION_SIZE = 4  # 4 direcciones: izquierda, abajo, derecha, arriba
    
    # NÃºmero mÃ¡ximo de pasos por episodio
    MAX_STEPS = 100
    
    # ConfiguraciÃ³n de renderizado
    RENDER_MODE = None  # None para entrenamiento, "human" para visualizaciÃ³n

# ================================
# HIPERPARÃMETROS DE LA RED NEURONAL
# ================================
class NetworkConfig:
    """ConfiguraciÃ³n de la red neuronal Q-Network"""
    
    # Arquitectura de la red
    INPUT_SIZE = EnvConfig.STATE_SIZE
    HIDDEN_SIZES = [128, 128, 64]  # Red mÃ¡s profunda para mejor aprendizaje
    OUTPUT_SIZE = EnvConfig.ACTION_SIZE
    
    # ConfiguraciÃ³n de entrenamiento
    LEARNING_RATE = 0.0005  # Learning rate mÃ¡s conservador
    BATCH_SIZE = 64  # Batch size mayor para estabilidad
    
    # RegularizaciÃ³n
    DROPOUT_RATE = 0.1  # Dropout ligero
    WEIGHT_DECAY = 1e-5

# ================================
# HIPERPARÃMETROS DEL AGENTE DQN
# ================================
class DQNConfig:
    """ConfiguraciÃ³n del agente DQN optimizada para rutas Ã³ptimas"""
    
    # ExploraciÃ³n (Epsilon-Greedy) - AJUSTADO PARA MEJOR EVALUACIÃ“N
    EPSILON_START = 1.0      # ExploraciÃ³n inicial mÃ¡xima
    EPSILON_END = 0.05       # ExploraciÃ³n mÃ­nima MÃS BAJA para mejor evaluaciÃ³n
    EPSILON_DECAY = 0.9997   # Factor de decaimiento MÃS LENTO
    
    # Q-Learning
    GAMMA = 0.99            # Factor de descuento
    TAU = 0.005             # Soft update para target network
    
    # Experience Replay
    MEMORY_SIZE = 50000     # Buffer mÃ¡s grande para mÃ¡s experiencias
    MIN_MEMORY_SIZE = 2000  # MÃ­nimo mÃ¡s alto para mejor estabilidad
    
    # Frecuencia de actualizaciÃ³n
    UPDATE_EVERY = 4        # Actualizar cada N pasos
    TARGET_UPDATE_EVERY = 100  # Actualizar target network cada N episodios

# ================================
# CONFIGURACIÃ“N DE ENTRENAMIENTO
# ================================
class TrainingConfig:
    """ConfiguraciÃ³n del proceso de entrenamiento extendido"""
    
    # DuraciÃ³n del entrenamiento - MÃS LARGO PARA MEJOR CONVERGENCIA
    NUM_EPISODES = 6000     # NÃºmero total de episodios (aumentado)
    
    # Logging y guardado
    PRINT_EVERY = 200       # Imprimir progreso cada N episodios
    SAVE_EVERY = 1000       # Guardar modelo cada N episodios
    
    # MÃ©tricas para evaluar convergencia
    SOLVE_SCORE = 0.8       # Tasa de Ã©xito para considerar resuelto
    SOLVE_WINDOW = 100      # Ventana de episodios para evaluar
    
    # Paths de archivos
    MODEL_PATH = "frozen_lake_dqn.pth"
    RESULTS_PATH = "training_results.png"
    LOG_PATH = "training_log.txt"

# ================================
# CONFIGURACIÃ“N DE EVALUACIÃ“N
# ================================
class EvalConfig:
    """ConfiguraciÃ³n para evaluaciÃ³n del agente"""
    
    # NÃºmero de episodios de evaluaciÃ³n
    EVAL_EPISODES = 1000
    
    # ConfiguraciÃ³n de renderizado para demos
    DEMO_EPISODES = 5
    RENDER_DEMO = True
    
    # MÃ©tricas objetivo
    TARGET_SUCCESS_RATE = 0.8     # Tasa de Ã©xito objetivo
    TARGET_OPTIMAL_RATE = 0.2     # Tasa de rutas Ã³ptimas objetivo (20%)
    OPTIMAL_STEPS = 6             # NÃºmero de pasos Ã³ptimo

# ================================
# CONFIGURACIÃ“N DE RECOMPENSAS
# ================================
class RewardConfig:
    """ConfiguraciÃ³n del sistema de recompensas moldeadas"""
    
    # Recompensas por Ã©xito
    BASE_SUCCESS_REWARD = 10.0      # Recompensa base por llegar a la meta
    OPTIMAL_BONUS = 100.0           # BonificaciÃ³n masiva por rutas Ã³ptimas (â‰¤6 pasos)
    NEAR_OPTIMAL_BONUS = 50.0       # BonificaciÃ³n por rutas casi-Ã³ptimas (â‰¤8 pasos)
    GOOD_BONUS = 20.0               # BonificaciÃ³n por rutas buenas (â‰¤12 pasos)
    
    # Bonificaciones por seguir rutas teÃ³ricas
    THEORETICAL_PATH_BONUS = 5.0    # BonificaciÃ³n por seguir ruta teÃ³rica exacta
    OPTIMAL_PATH_STEP_BONUS = 0.5   # BonificaciÃ³n por estar en ruta Ã³ptima
    
    # Recompensas de navegaciÃ³n
    PROGRESS_REWARD = 1.0           # Recompensa por acercarse a la meta
    REGRESS_PENALTY = -0.5          # PenalizaciÃ³n por alejarse
    NO_PROGRESS_PENALTY = -0.1      # PenalizaciÃ³n por no progresar
    
    # Penalizaciones
    HOLE_PENALTY = -5.0             # PenalizaciÃ³n por caer en agujero
    STEP_PENALTY_THRESHOLD = 15     # Umbral para penalizaciÃ³n por pasos excesivos
    STEP_PENALTY_RATE = 0.1         # PenalizaciÃ³n por paso excesivo

# ================================
# RUTAS Ã“PTIMAS CONOCIDAS
# ================================
class OptimalPaths:
    """Rutas Ã³ptimas conocidas para el mapa 4x4"""
    
    PATHS_4X4 = [
        [0, 1, 2, 6, 10, 14, 15],    # Ruta Norte-Este
        [0, 4, 8, 9, 10, 14, 15],    # Ruta Sur-Este (variante 1)
        [0, 4, 8, 9, 13, 14, 15],    # Ruta Sur-Este (variante 2)
    ]
    
    @classmethod
    def get_optimal_actions(cls, state, path):
        """
        Obtener la acciÃ³n Ã³ptima para un estado dado en una ruta especÃ­fica
        
        Args:
            state (int): Estado actual
            path (list): Ruta Ã³ptima como lista de estados
            
        Returns:
            int or None: AcciÃ³n Ã³ptima (0=izq, 1=abajo, 2=der, 3=arriba) o None
        """
        if state not in path:
            return None
            
        current_idx = path.index(state)
        if current_idx >= len(path) - 1:
            return None
            
        next_state = path[current_idx + 1]
        
        # Calcular acciÃ³n basada en la diferencia de posiciones
        current_row, current_col = divmod(state, 4)
        next_row, next_col = divmod(next_state, 4)
        
        if next_row > current_row:
            return 1  # Abajo
        elif next_row < current_row:
            return 3  # Arriba
        elif next_col > current_col:
            return 2  # Derecha
        elif next_col < current_col:
            return 0  # Izquierda
        
        return None
    
    @classmethod
    def is_on_optimal_path(cls, state):
        """Verificar si un estado estÃ¡ en alguna ruta Ã³ptima"""
        for path in cls.PATHS_4X4:
            if state in path:
                return True
        return False
    
    @classmethod
    def check_path_match(cls, full_path):
        """
        Verificar si un camino completo coincide con alguna ruta teÃ³rica
        
        Args:
            full_path (list): Camino completo como lista de estados
            
        Returns:
            int or None: Ãndice de la ruta teÃ³rica coincidente o None
        """
        for i, theoretical_path in enumerate(cls.PATHS_4X4):
            if full_path == theoretical_path:
                return i
        return None

# ================================
# CONFIGURACIÃ“N DE LOGGING
# ================================
class LogConfig:
    """ConfiguraciÃ³n de logging y visualizaciÃ³n"""
    
    # Colores para terminal
    COLORS = {
        'SUCCESS': '\033[92m',    # Verde
        'WARNING': '\033[93m',    # Amarillo
        'ERROR': '\033[91m',      # Rojo
        'INFO': '\033[94m',       # Azul
        'BOLD': '\033[1m',        # Negrita
        'END': '\033[0m'          # Fin de color
    }
    
    # Emojis para diferentes tipos de mensajes
    EMOJIS = {
        'SUCCESS': 'âœ…',
        'OPTIMAL': 'ðŸŽ¯',
        'WARNING': 'âš ï¸',
        'ERROR': 'âŒ',
        'INFO': 'â„¹ï¸',
        'TRAIN': 'ðŸš€',
        'EVAL': 'ðŸ”¬',
        'SAVE': 'ðŸ’¾',
        'THEORETICAL': 'ðŸ†'
    }

# ================================
# UTILIDADES DE CONFIGURACIÃ“N
# ================================
def print_config():
    """Imprimir la configuraciÃ³n actual"""
    print(f"{LogConfig.COLORS['BOLD']}=== CONFIGURACIÃ“N DQN FROZEN LAKE MEJORADO ==={LogConfig.COLORS['END']}")
    print(f"Dispositivo: {DEVICE}")
    print(f"Mapa: {EnvConfig.MAP_SIZE} ({'resbaladizo' if EnvConfig.IS_SLIPPERY else 'determinÃ­stico'})")
    print(f"Episodios: {TrainingConfig.NUM_EPISODES}")
    print(f"Arquitectura: {NetworkConfig.INPUT_SIZE} â†’ {' â†’ '.join(map(str, NetworkConfig.HIDDEN_SIZES))} â†’ {NetworkConfig.OUTPUT_SIZE}")
    print(f"Learning Rate: {NetworkConfig.LEARNING_RATE}")
    print(f"Epsilon: {DQNConfig.EPSILON_START} â†’ {DQNConfig.EPSILON_END} (decay: {DQNConfig.EPSILON_DECAY})")
    print(f"Gamma: {DQNConfig.GAMMA}")
    print(f"Batch Size: {NetworkConfig.BATCH_SIZE}")
    print(f"Recompensas moldeadas: âœ…")
    print(f"ExploraciÃ³n dirigida: âœ…")
    print("-" * 50)

def validate_config():
    """Validar que la configuraciÃ³n sea coherente"""
    assert EnvConfig.STATE_SIZE == 16, "STATE_SIZE debe ser 16 para mapa 4x4"
    assert EnvConfig.ACTION_SIZE == 4, "ACTION_SIZE debe ser 4"
    assert 0 < DQNConfig.EPSILON_DECAY < 1, "EPSILON_DECAY debe estar entre 0 y 1"
    assert 0 < DQNConfig.GAMMA <= 1, "GAMMA debe estar entre 0 y 1"
    assert NetworkConfig.BATCH_SIZE <= DQNConfig.MEMORY_SIZE, "BATCH_SIZE no puede ser mayor que MEMORY_SIZE"
    assert DQNConfig.MIN_MEMORY_SIZE <= DQNConfig.MEMORY_SIZE, "MIN_MEMORY_SIZE no puede ser mayor que MEMORY_SIZE"
    assert EvalConfig.OPTIMAL_STEPS <= EnvConfig.MAX_STEPS, "OPTIMAL_STEPS debe ser menor que MAX_STEPS"
    
    # Validar que las rutas Ã³ptimas sean vÃ¡lidas
    for i, path in enumerate(OptimalPaths.PATHS_4X4):
        assert len(path) == 7, f"Ruta {i+1} debe tener 7 estados (6 pasos)"
        assert path[0] == 0, f"Ruta {i+1} debe empezar en estado 0"
        assert path[-1] == 15, f"Ruta {i+1} debe terminar en estado 15"
        assert all(0 <= state <= 15 for state in path), f"Ruta {i+1} contiene estados invÃ¡lidos"
    
    print(f"{LogConfig.EMOJIS['SUCCESS']} ConfiguraciÃ³n validada correctamente")

def get_distance_to_goal(state):
    """Calcular distancia Manhattan desde un estado a la meta (estado 15)"""
    goal_row, goal_col = 3, 3
    state_row, state_col = divmod(state, 4)
    return abs(state_row - goal_row) + abs(state_col - goal_col)

if __name__ == "__main__":
    print_config()
    validate_config()
    
    # Mostrar rutas Ã³ptimas
    print(f"\n{LogConfig.EMOJIS['OPTIMAL']} Rutas Ã³ptimas objetivo:")
    for i, path in enumerate(OptimalPaths.PATHS_4X4, 1):
        path_str = " â†’ ".join(map(str, path))
        print(f"  {i}. {path_str}")
    
    print(f"\n{LogConfig.EMOJIS['INFO']} ConfiguraciÃ³n de recompensas:")
    print(f"  Ruta Ã³ptima (â‰¤6 pasos): +{RewardConfig.BASE_SUCCESS_REWARD + RewardConfig.OPTIMAL_BONUS}")
    print(f"  Ruta casi-Ã³ptima (â‰¤8 pasos): +{RewardConfig.BASE_SUCCESS_REWARD + RewardConfig.NEAR_OPTIMAL_BONUS}")
    print(f"  Seguir ruta teÃ³rica: +{RewardConfig.THEORETICAL_PATH_BONUS}")
    print(f"  Progreso hacia meta: +{RewardConfig.PROGRESS_REWARD}")