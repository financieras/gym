"""
config.py - ConfiguraciÃ³n e HiperparÃ¡metros para DQN en Frozen Lake

Este archivo contiene todas las configuraciones y hiperparÃ¡metros necesarios
para entrenar el agente DQN en el entorno Frozen Lake.
"""

import torch

# ================================
# CONFIGURACIÃ“N DEL DISPOSITIVO
# ================================
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ================================
# CONFIGURACIÃ“N DEL ENTORNO
# ================================
class EnvConfig:
    """ConfiguraciÃ³n del entorno Frozen Lake"""
    
    # ConfiguraciÃ³n del mapa
    MAP_SIZE = "4x4"  # "4x4" o "8x8"
    IS_SLIPPERY = True  # True para entorno estocÃ¡stico, False para determinÃ­stico
    
    # Espacios de estado y acciÃ³n
    STATE_SIZE = 16  # Para mapa 4x4
    ACTION_SIZE = 4  # 4 direcciones: izquierda, abajo, derecha, arriba
    
    # NÃºmero mÃ¡ximo de pasos por episodio
    MAX_STEPS = 100
    
    # ConfiguraciÃ³n de renderizado
    RENDER_MODE = None  # None para entrenamiento, "human" para visualizaciÃ³n

# ================================
# HIPERPARÃMETROS DE LA RED NEURONAL
# ================================
class NetworkConfig:
    """ConfiguraciÃ³n de la red neuronal Q-Network"""
    
    # Arquitectura de la red
    INPUT_SIZE = EnvConfig.STATE_SIZE
    HIDDEN_SIZES = [64, 64]  # Capas ocultas mÃ¡s simples
    OUTPUT_SIZE = EnvConfig.ACTION_SIZE
    
    # ConfiguraciÃ³n de entrenamiento
    LEARNING_RATE = 0.001
    BATCH_SIZE = 32
    
    # RegularizaciÃ³n
    DROPOUT_RATE = 0.0  # Sin dropout inicialmente
    WEIGHT_DECAY = 1e-5

# ================================
# HIPERPARÃMETROS DEL AGENTE DQN
# ================================
class DQNConfig:
    """ConfiguraciÃ³n del agente DQN"""
    
    # ExploraciÃ³n (Epsilon-Greedy)
    EPSILON_START = 1.0      # ExploraciÃ³n inicial mÃ¡xima
    EPSILON_END = 0.01       # ExploraciÃ³n mÃ­nima
    EPSILON_DECAY = 0.995    # Factor de decaimiento por episodio
    
    # Q-Learning
    GAMMA = 0.99            # Factor de descuento
    TAU = 0.005             # Soft update para target network
    
    # Experience Replay
    MEMORY_SIZE = 10000     # TamaÃ±o del buffer de memoria
    MIN_MEMORY_SIZE = 1000  # MÃ­nimo para comenzar entrenamiento
    
    # Frecuencia de actualizaciÃ³n
    UPDATE_EVERY = 4        # Actualizar cada N pasos
    TARGET_UPDATE_EVERY = 100  # Actualizar target network cada N episodios

# ================================
# CONFIGURACIÃ“N DE ENTRENAMIENTO
# ================================
class TrainingConfig:
    """ConfiguraciÃ³n del proceso de entrenamiento"""
    
    # DuraciÃ³n del entrenamiento
    NUM_EPISODES = 2000     # NÃºmero total de episodios
    
    # Logging y guardado
    PRINT_EVERY = 100       # Imprimir progreso cada N episodios
    SAVE_EVERY = 500        # Guardar modelo cada N episodios
    
    # MÃ©tricas para evaluar convergencia
    SOLVE_SCORE = 0.8       # Tasa de Ã©xito para considerar resuelto
    SOLVE_WINDOW = 100      # Ventana de episodios para evaluar
    
    # Paths de archivos
    MODEL_PATH = "frozen_lake_dqn.pth"
    RESULTS_PATH = "training_results.png"
    LOG_PATH = "training_log.txt"

# ================================
# CONFIGURACIÃ“N DE EVALUACIÃ“N
# ================================
class EvalConfig:
    """ConfiguraciÃ³n para evaluaciÃ³n del agente"""
    
    # NÃºmero de episodios de evaluaciÃ³n
    EVAL_EPISODES = 1000
    
    # ConfiguraciÃ³n de renderizado para demos
    DEMO_EPISODES = 5
    RENDER_DEMO = True
    
    # MÃ©tricas objetivo
    TARGET_SUCCESS_RATE = 0.8     # Tasa de Ã©xito objetivo
    TARGET_OPTIMAL_RATE = 0.3     # Tasa de rutas Ã³ptimas objetivo
    OPTIMAL_STEPS = 6             # NÃºmero de pasos Ã³ptimo

# ================================
# RUTAS Ã“PTIMAS CONOCIDAS
# ================================
class OptimalPaths:
    """Rutas Ã³ptimas conocidas para el mapa 4x4"""
    
    PATHS_4X4 = [
        [0, 1, 2, 6, 10, 14, 15],    # Ruta Norte-Este
        [0, 4, 8, 9, 10, 14, 15],    # Ruta Sur-Este (variante 1)
        [0, 4, 8, 9, 13, 14, 15],    # Ruta Sur-Este (variante 2)
    ]
    
    @classmethod
    def get_optimal_actions(cls, state, path):
        """
        Obtener la acciÃ³n Ã³ptima para un estado dado en una ruta especÃ­fica
        
        Args:
            state (int): Estado actual
            path (list): Ruta Ã³ptima como lista de estados
            
        Returns:
            int or None: AcciÃ³n Ã³ptima (0=izq, 1=abajo, 2=der, 3=arriba) o None
        """
        if state not in path:
            return None
            
        current_idx = path.index(state)
        if current_idx >= len(path) - 1:
            return None
            
        next_state = path[current_idx + 1]
        
        # Calcular acciÃ³n basada en la diferencia de posiciones
        current_row, current_col = divmod(state, 4)
        next_row, next_col = divmod(next_state, 4)
        
        if next_row > current_row:
            return 1  # Abajo
        elif next_row < current_row:
            return 3  # Arriba
        elif next_col > current_col:
            return 2  # Derecha
        elif next_col < current_col:
            return 0  # Izquierda
        
        return None

# ================================
# CONFIGURACIÃ“N DE LOGGING
# ================================
class LogConfig:
    """ConfiguraciÃ³n de logging y visualizaciÃ³n"""
    
    # Colores para terminal
    COLORS = {
        'SUCCESS': '\033[92m',    # Verde
        'WARNING': '\033[93m',    # Amarillo
        'ERROR': '\033[91m',      # Rojo
        'INFO': '\033[94m',       # Azul
        'BOLD': '\033[1m',        # Negrita
        'END': '\033[0m'          # Fin de color
    }
    
    # Emojis para diferentes tipos de mensajes
    EMOJIS = {
        'SUCCESS': 'âœ…',
        'OPTIMAL': 'ðŸŽ¯',
        'WARNING': 'âš ï¸',
        'ERROR': 'âŒ',
        'INFO': 'â„¹ï¸',
        'TRAIN': 'ðŸš€',
        'EVAL': 'ðŸ”¬',
        'SAVE': 'ðŸ’¾'
    }

# ================================
# UTILIDADES DE CONFIGURACIÃ“N
# ================================
def print_config():
    """Imprimir la configuraciÃ³n actual"""
    print(f"{LogConfig.COLORS['BOLD']}=== CONFIGURACIÃ“N DQN FROZEN LAKE ==={LogConfig.COLORS['END']}")
    print(f"Dispositivo: {DEVICE}")
    print(f"Mapa: {EnvConfig.MAP_SIZE} ({'resbaladizo' if EnvConfig.IS_SLIPPERY else 'determinÃ­stico'})")
    print(f"Episodios: {TrainingConfig.NUM_EPISODES}")
    print(f"Arquitectura: {NetworkConfig.INPUT_SIZE} â†’ {' â†’ '.join(map(str, NetworkConfig.HIDDEN_SIZES))} â†’ {NetworkConfig.OUTPUT_SIZE}")
    print(f"Learning Rate: {NetworkConfig.LEARNING_RATE}")
    print(f"Epsilon: {DQNConfig.EPSILON_START} â†’ {DQNConfig.EPSILON_END} (decay: {DQNConfig.EPSILON_DECAY})")
    print(f"Gamma: {DQNConfig.GAMMA}")
    print(f"Batch Size: {NetworkConfig.BATCH_SIZE}")
    print("-" * 50)

def validate_config():
    """Validar que la configuraciÃ³n sea coherente"""
    assert EnvConfig.STATE_SIZE == 16, "STATE_SIZE debe ser 16 para mapa 4x4"
    assert EnvConfig.ACTION_SIZE == 4, "ACTION_SIZE debe ser 4"
    assert 0 < DQNConfig.EPSILON_DECAY < 1, "EPSILON_DECAY debe estar entre 0 y 1"
    assert 0 < DQNConfig.GAMMA <= 1, "GAMMA debe estar entre 0 y 1"
    assert NetworkConfig.BATCH_SIZE <= DQNConfig.MEMORY_SIZE, "BATCH_SIZE no puede ser mayor que MEMORY_SIZE"
    assert DQNConfig.MIN_MEMORY_SIZE <= DQNConfig.MEMORY_SIZE, "MIN_MEMORY_SIZE no puede ser mayor que MEMORY_SIZE"
    
    print(f"{LogConfig.EMOJIS['SUCCESS']} ConfiguraciÃ³n validada correctamente")

if __name__ == "__main__":
    print_config()
    validate_config()