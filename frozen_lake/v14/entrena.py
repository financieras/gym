import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
import time

class QLearningAgent:
    """
    Agente que implementa el algoritmo Q-Learning para resolver el problema de Frozen Lake.
    
    Q-Learning es un algoritmo de aprendizaje por refuerzo que aprende una funci√≥n Q(s,a)
    que representa el valor esperado de tomar la acci√≥n 'a' en el estado 's'.
    """
    
    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):
        """
        Inicializa el agente Q-Learning.
        
        Par√°metros:
        - env: Entorno de gymnasium (Frozen Lake)
        - learning_rate (alpha): Tasa de aprendizaje (0 < Œ± ‚â§ 1)
        - discount_factor (gamma): Factor de descuento para recompensas futuras (0 ‚â§ Œ≥ ‚â§ 1)
        - epsilon: Probabilidad inicial de exploraci√≥n (estrategia Œµ-greedy)
        - epsilon_decay: Factor de decaimiento de epsilon
        - epsilon_min: Valor m√≠nimo de epsilon
        """
        self.env = env
        self.n_states = env.observation_space.n  # N√∫mero de estados (16 para mapa 4x4)
        self.n_actions = env.action_space.n      # N√∫mero de acciones (4: izquierda, abajo, derecha, arriba)
        
        # Par√°metros del algoritmo Q-Learning
        self.learning_rate = learning_rate      # Œ± (alpha)
        self.discount_factor = discount_factor  # Œ≥ (gamma)
        self.epsilon = epsilon                  # Œµ (epsilon) para estrategia Œµ-greedy
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        
        # Inicializar la Q-table con valores cero
        # Q-table[estado][acci√≥n] = valor Q
        self.q_table = np.zeros((self.n_states, self.n_actions))
        
        print(f"üßä Agente Q-Learning inicializado:")
        print(f"   - Estados: {self.n_states}")
        print(f"   - Acciones: {self.n_actions}")
        print(f"   - Tasa de aprendizaje (Œ±): {self.learning_rate}")
        print(f"   - Factor de descuento (Œ≥): {self.discount_factor}")
        print(f"   - Epsilon inicial (Œµ): {self.epsilon}")
    
    def choose_action(self, state):
        """
        Selecciona una acci√≥n usando la estrategia Œµ-greedy:
        - Con probabilidad Œµ: explora (acci√≥n aleatoria)
        - Con probabilidad (1-Œµ): explota (mejor acci√≥n conocida)
        """
        if np.random.random() < self.epsilon:
            # Exploraci√≥n: elegir acci√≥n aleatoria
            return self.env.action_space.sample()
        else:
            # Explotaci√≥n: elegir la mejor acci√≥n conocida (mayor valor Q)
            return np.argmax(self.q_table[state])
    
    def update_q_value(self, state, action, reward, next_state):
        """
        Actualiza el valor Q usando la ecuaci√≥n de Bellman:
        
        Q(s,a) = Q(s,a) + Œ± * [r + Œ≥ * max(Q(s',a')) - Q(s,a)]
        
        Donde:
        - Q(s,a): Valor Q actual para el estado s y acci√≥n a
        - Œ± (alpha): Tasa de aprendizaje
        - r: Recompensa inmediata
        - Œ≥ (gamma): Factor de descuento
        - max(Q(s',a')): Mejor valor Q posible en el siguiente estado
        """
        # Valor Q actual
        current_q = self.q_table[state, action]
        
        # Mejor valor Q posible en el siguiente estado
        max_next_q = np.max(self.q_table[next_state])
        
        # Calcular el nuevo valor Q usando la ecuaci√≥n de Bellman
        new_q = current_q + self.learning_rate * (reward + self.discount_factor * max_next_q - current_q)
        
        # Actualizar la Q-table
        self.q_table[state, action] = new_q
    
    def decay_epsilon(self):
        """
        Reduce gradualmente epsilon para disminuir la exploraci√≥n con el tiempo.
        Al principio exploramos mucho, pero gradualmente explotamos m√°s el conocimiento adquirido.
        """
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

def train_agent(episodes=1000, render_last_episodes=5):
    """
    Entrena al agente Q-Learning en el entorno Frozen Lake.
    
    Par√°metros:
    - episodes: N√∫mero de episodios de entrenamiento
    - render_last_episodes: N√∫mero de episodios finales a mostrar visualmente
    """
    # Crear el entorno Frozen Lake
    # is_slippery=True significa que el lago es resbaladizo (m√°s realista y desafiante)
    env = gym.make('FrozenLake-v1', desc=None, map_name="4x4", is_slippery=True, render_mode=None)
    
    # Crear el agente Q-Learning
    agent = QLearningAgent(env)
    
    # M√©tricas para seguimiento del progreso
    rewards_per_episode = []
    success_rate_window = []
    window_size = 100  # Ventana para calcular tasa de √©xito
    
    print(f"\nüöÄ Iniciando entrenamiento por {episodes} episodios...")
    print("=" * 60)
    
    # Bucle principal de entrenamiento
    for episode in range(episodes):
        # Reiniciar el entorno para un nuevo episodio
        state, _ = env.reset()
        total_reward = 0
        steps = 0
        done = False
        
        # Bucle del episodio
        while not done:
            # El agente elige una acci√≥n
            action = agent.choose_action(state)
            
            # Ejecutar la acci√≥n en el entorno
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            # Actualizar el valor Q usando la ecuaci√≥n de Bellman
            agent.update_q_value(state, action, reward, next_state)
            
            # Actualizar estado y m√©tricas
            state = next_state
            total_reward += reward
            steps += 1
        
        # Reducir epsilon (menos exploraci√≥n con el tiempo)
        agent.decay_epsilon()
        
        # Guardar m√©tricas
        rewards_per_episode.append(total_reward)
        success_rate_window.append(total_reward > 0)  # √âxito si lleg√≥ a la meta
        
        # Mantener ventana deslizante para tasa de √©xito
        if len(success_rate_window) > window_size:
            success_rate_window.pop(0)
        
        # Mostrar progreso cada 100 episodios
        if (episode + 1) % 100 == 0:
            success_rate = np.mean(success_rate_window) * 100
            print(f"Episodio {episode + 1:4d} | "
                  f"Epsilon: {agent.epsilon:.3f} | "
                  f"Tasa √©xito √∫ltimos {len(success_rate_window)} ep.: {success_rate:.1f}% | "
                  f"Pasos: {steps:2d} | "
                  f"Recompensa: {total_reward:.1f}")
    
    env.close()
    
    # Mostrar algunos episodios finales con visualizaci√≥n
    print(f"\nüé¨ Mostrando los √∫ltimos {render_last_episodes} episodios con visualizaci√≥n...")
    test_agent(agent, episodes=render_last_episodes, render=True)
    
    # Mostrar estad√≠sticas finales
    print_final_statistics(agent, rewards_per_episode)
    
    # Graficar progreso del entrenamiento
    plot_training_progress(rewards_per_episode)
    
    return agent

def test_agent(agent, episodes=100, render=False):
    """
    Eval√∫a el rendimiento del agente entrenado.
    """
    render_mode = 'human' if render else None
    env = gym.make('FrozenLake-v1', desc=None, map_name="4x4", is_slippery=True, render_mode=render_mode)
    
    successes = 0
    total_steps = 0
    
    for episode in range(episodes):
        state, _ = env.reset()
        steps = 0
        done = False
        
        while not done:
            # En evaluaci√≥n, siempre elegir la mejor acci√≥n (sin exploraci√≥n)
            action = np.argmax(agent.q_table[state])
            state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            steps += 1
            
            if render:
                time.sleep(0.1)  # Pausa para visualizaci√≥n
        
        if reward > 0:  # Lleg√≥ a la meta
            successes += 1
        
        total_steps += steps
        
        if render:
            print(f"Episodio {episode + 1}: {'‚úÖ √âxito' if reward > 0 else '‚ùå Fallo'} en {steps} pasos")
    
    env.close()
    
    success_rate = (successes / episodes) * 100
    avg_steps = total_steps / episodes
    
    if not render:
        print(f"\nüìä Evaluaci√≥n completada:")
        print(f"   - Tasa de √©xito: {success_rate:.1f}% ({successes}/{episodes})")
        print(f"   - Pasos promedio: {avg_steps:.1f}")
    
    return success_rate, avg_steps

def print_final_statistics(agent, rewards_per_episode):
    """
    Muestra estad√≠sticas finales del entrenamiento y la Q-table aprendida.
    """
    print("\n" + "=" * 60)
    print("üìà ESTAD√çSTICAS FINALES DEL ENTRENAMIENTO")
    print("=" * 60)
    
    total_episodes = len(rewards_per_episode)
    successful_episodes = sum(1 for r in rewards_per_episode if r > 0)
    success_rate = (successful_episodes / total_episodes) * 100
    
    print(f"Episodios totales: {total_episodes}")
    print(f"Episodios exitosos: {successful_episodes}")
    print(f"Tasa de √©xito final: {success_rate:.2f}%")
    print(f"Epsilon final: {agent.epsilon:.4f}")
    
    print(f"\nüß† Q-TABLE APRENDIDA:")
    print("Cada fila representa un estado, cada columna una acci√≥n [Izq, Abajo, Der, Arriba]")
    print("-" * 40)
    for state in range(agent.n_states):
        row = state // 4
        col = state % 4
        print(f"Estado {state:2d} (fila {row}, col {col}): {agent.q_table[state]}")

def plot_training_progress(rewards_per_episode):
    """
    Grafica el progreso del entrenamiento.
    """
    # Calcular media m√≥vil para suavizar la curva
    window_size = 100
    if len(rewards_per_episode) >= window_size:
        moving_avg = []
        for i in range(len(rewards_per_episode) - window_size + 1):
            moving_avg.append(np.mean(rewards_per_episode[i:i + window_size]))
        
        plt.figure(figsize=(12, 5))
        
        # Subplot 1: Recompensas por episodio
        plt.subplot(1, 2, 1)
        plt.plot(rewards_per_episode, alpha=0.3, label='Recompensa por episodio')
        plt.plot(range(window_size-1, len(rewards_per_episode)), moving_avg, 
                label=f'Media m√≥vil ({window_size} episodios)', linewidth=2)
        plt.xlabel('Episodio')
        plt.ylabel('Recompensa')
        plt.title('Progreso del Entrenamiento')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # Subplot 2: Tasa de √©xito acumulativa
        plt.subplot(1, 2, 2)
        cumulative_success = np.cumsum([1 if r > 0 else 0 for r in rewards_per_episode])
        success_rate = cumulative_success / np.arange(1, len(rewards_per_episode) + 1) * 100
        plt.plot(success_rate)
        plt.xlabel('Episodio')
        plt.ylabel('Tasa de √âxito (%)')
        plt.title('Tasa de √âxito Acumulativa')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()

def main():
    """
    Funci√≥n principal que ejecuta el entrenamiento completo.
    """
    print("üßä FROZEN LAKE con Q-LEARNING ü§ñ")
    print("=" * 50)
    print("Este programa implementa el algoritmo Q-Learning para resolver")
    print("el problema de Frozen Lake, donde un agente debe navegar por")
    print("un lago congelado resbaladizo desde el inicio hasta la meta,")
    print("evitando caer en los agujeros.")
    print()
    print("Leyenda del mapa:")
    print("  S = Inicio (Start)")
    print("  F = Superficie congelada (Frozen)")
    print("  H = Agujero (Hole)")
    print("  G = Meta (Goal)")
    print()
    
    # Entrenar el agente
    trained_agent = train_agent(episodes=2000, render_last_episodes=3)
    
    # Evaluaci√≥n final sin visualizaci√≥n
    print(f"\nüéØ Evaluaci√≥n final con 1000 episodios...")
    final_success_rate, final_avg_steps = test_agent(trained_agent, episodes=1000, render=False)
    
    print(f"\nüèÜ RESULTADO FINAL:")
    print(f"   El agente alcanz√≥ una tasa de √©xito del {final_success_rate:.1f}%")
    print(f"   con un promedio de {final_avg_steps:.1f} pasos por episodio.")
    
    if final_success_rate > 70:
        print("   ¬°Excelente! El agente ha aprendido una buena pol√≠tica. üéâ")
    elif final_success_rate > 50:
        print("   ¬°Bien! El agente ha aprendido una pol√≠tica decente. üëç")
    else:
        print("   El agente necesita m√°s entrenamiento. ü§î")

# Ejecutar el programa principal
if __name__ == "__main__":
    main()