"""
config.py - Configuraci√≥n para Q-Learning con Q-Table en Frozen Lake

Este archivo contiene la configuraci√≥n optimizada para Q-Learning cl√°sico
con tabla Q en lugar de redes neuronales, para mejor convergencia en
espacios de estados peque√±os como Frozen Lake 4x4.
"""

import numpy as np

# ================================
# CONFIGURACI√ìN DEL ENTORNO
# ================================
class EnvConfig:
    """Configuraci√≥n del entorno Frozen Lake"""
    
    MAP_SIZE = "4x4"
    IS_SLIPPERY = True
    STATE_SIZE = 16  # Estados 0-15
    ACTION_SIZE = 4  # Izquierda, Abajo, Derecha, Arriba
    MAX_STEPS = 100
    RENDER_MODE = None

# ================================
# HIPERPAR√ÅMETROS DE Q-LEARNING
# ================================
class QLearningConfig:
    """Configuraci√≥n de Q-Learning con tabla Q"""
    
    # Par√°metros de aprendizaje
    LEARNING_RATE = 0.1        # Alpha - tasa de aprendizaje
    DISCOUNT_FACTOR = 0.99     # Gamma - factor de descuento
    
    # Exploraci√≥n (Epsilon-Greedy)
    EPSILON_START = 1.0        # Exploraci√≥n inicial m√°xima
    EPSILON_END = 0.01         # Exploraci√≥n m√≠nima final
    EPSILON_DECAY = 0.9995     # Decay muy gradual
    
    # Configuraci√≥n de convergencia
    MIN_LEARNING_RATE = 0.01   # Learning rate m√≠nimo
    LR_DECAY = 0.9999          # Decay del learning rate

# ================================
# CONFIGURACI√ìN DE ENTRENAMIENTO
# ================================
class TrainingConfig:
    """Configuraci√≥n del entrenamiento"""
    
    NUM_EPISODES = 8000        # M√°s episodios para convergencia completa
    PRINT_EVERY = 400          # Reportar cada 400 episodios
    SAVE_EVERY = 2000          # Guardar cada 2000 episodios
    
    # Criterios de convergencia
    SOLVE_SCORE = 0.8          # Tasa de √©xito objetivo
    SOLVE_WINDOW = 100         # Ventana para evaluar convergencia
    
    # Archivos
    MODEL_PATH = "frozen_lake_qtable.npy"
    RESULTS_PATH = "qtable_results.png"

# ================================
# CONFIGURACI√ìN DE EVALUACI√ìN
# ================================
class EvalConfig:
    """Configuraci√≥n para evaluaci√≥n"""
    
    EVAL_EPISODES = 1000
    DEMO_EPISODES = 5
    
    # Objetivos
    TARGET_SUCCESS_RATE = 0.8
    TARGET_OPTIMAL_RATE = 0.3    # 30% de rutas √≥ptimas
    OPTIMAL_STEPS = 6

# ================================
# CONFIGURACI√ìN DE RECOMPENSAS
# ================================
class RewardConfig:
    """Sistema de recompensas moldeadas para Q-Learning"""
    
    # Recompensas principales
    SUCCESS_REWARD = 100.0          # Llegar a la meta
    OPTIMAL_BONUS = 200.0           # Bonificaci√≥n por ruta √≥ptima
    THEORETICAL_BONUS = 50.0        # Bonificaci√≥n por ruta te√≥rica exacta
    
    # Recompensas de navegaci√≥n
    PROGRESS_REWARD = 1.0           # Acercarse a la meta
    REGRESS_PENALTY = -0.5          # Alejarse de la meta
    HOLE_PENALTY = -10.0            # Caer en agujero
    STEP_PENALTY = -0.1             # Penalizaci√≥n por paso
    
    # Exploraci√≥n dirigida
    EXPLORATION_BONUS = 0.5         # Bonificaci√≥n por estados poco visitados

# ================================
# RUTAS √ìPTIMAS CONOCIDAS
# ================================
class OptimalPaths:
    """Rutas √≥ptimas conocidas y funciones de an√°lisis"""
    
    PATHS_4X4 = [
        [0, 1, 2, 6, 10, 14, 15],    # Ruta Norte-Este
        [0, 4, 8, 9, 10, 14, 15],    # Ruta Sur-Este (variante 1)
        [0, 4, 8, 9, 13, 14, 15],    # Ruta Sur-Este (variante 2)
    ]
    
    @classmethod
    def get_optimal_action(cls, state):
        """
        Obtener acci√≥n √≥ptima para un estado dado
        
        Args:
            state (int): Estado actual
            
        Returns:
            int or None: Acci√≥n √≥ptima o None si no est√° en ruta √≥ptima
        """
        for path in cls.PATHS_4X4:
            if state in path:
                current_idx = path.index(state)
                if current_idx < len(path) - 1:
                    next_state = path[current_idx + 1]
                    return cls._calculate_action(state, next_state)
        return None
    
    @classmethod
    def _calculate_action(cls, from_state, to_state):
        """Calcular acci√≥n necesaria para ir de un estado a otro"""
        from_row, from_col = divmod(from_state, 4)
        to_row, to_col = divmod(to_state, 4)
        
        if to_row > from_row:
            return 1  # Abajo
        elif to_row < from_row:
            return 3  # Arriba
        elif to_col > from_col:
            return 2  # Derecha
        elif to_col < from_col:
            return 0  # Izquierda
        
        return None
    
    @classmethod
    def is_optimal_path(cls, path):
        """Verificar si un camino es una ruta √≥ptima te√≥rica"""
        for i, theoretical_path in enumerate(cls.PATHS_4X4):
            if path == theoretical_path:
                return i
        return None
    
    @classmethod
    def is_on_optimal_path(cls, state):
        """Verificar si un estado est√° en alguna ruta √≥ptima"""
        for path in cls.PATHS_4X4:
            if state in path:
                return True
        return False

# ================================
# CONFIGURACI√ìN DE LOGGING
# ================================
class LogConfig:
    """Configuraci√≥n de logging y visualizaci√≥n"""
    
    COLORS = {
        'SUCCESS': '\033[92m',
        'WARNING': '\033[93m',
        'ERROR': '\033[91m',
        'INFO': '\033[94m',
        'BOLD': '\033[1m',
        'END': '\033[0m'
    }
    
    EMOJIS = {
        'SUCCESS': '‚úÖ',
        'OPTIMAL': 'üéØ',
        'WARNING': '‚ö†Ô∏è',
        'ERROR': '‚ùå',
        'INFO': '‚ÑπÔ∏è',
        'TRAIN': 'üöÄ',
        'EVAL': 'üî¨',
        'SAVE': 'üíæ',
        'QTABLE': 'üìä',
        'THEORETICAL': 'üèÜ'
    }

# ================================
# FUNCIONES DE UTILIDAD
# ================================
def get_manhattan_distance(state, goal_state=15):
    """Calcular distancia Manhattan entre dos estados"""
    state_row, state_col = divmod(state, 4)
    goal_row, goal_col = divmod(goal_state, 4)
    return abs(state_row - goal_row) + abs(state_col - goal_col)

def print_config():
    """Imprimir configuraci√≥n actual"""
    print(f"{LogConfig.COLORS['BOLD']}=== CONFIGURACI√ìN Q-LEARNING FROZEN LAKE ==={LogConfig.COLORS['END']}")
    print(f"Algoritmo: Q-Learning con Tabla Q")
    print(f"Estados: {EnvConfig.STATE_SIZE}")
    print(f"Acciones: {EnvConfig.ACTION_SIZE}")
    print(f"Episodios: {TrainingConfig.NUM_EPISODES}")
    print(f"Learning Rate: {QLearningConfig.LEARNING_RATE} ‚Üí {QLearningConfig.MIN_LEARNING_RATE}")
    print(f"Epsilon: {QLearningConfig.EPSILON_START} ‚Üí {QLearningConfig.EPSILON_END}")
    print(f"Gamma: {QLearningConfig.DISCOUNT_FACTOR}")
    print(f"Recompensas moldeadas: ‚úÖ")
    print(f"Exploraci√≥n dirigida: ‚úÖ")
    print("-" * 50)

def validate_config():
    """Validar configuraci√≥n"""
    assert EnvConfig.STATE_SIZE == 16, "STATE_SIZE debe ser 16 para mapa 4x4"
    assert EnvConfig.ACTION_SIZE == 4, "ACTION_SIZE debe ser 4"
    assert 0 < QLearningConfig.LEARNING_RATE <= 1, "Learning rate debe estar entre 0 y 1"
    assert 0 < QLearningConfig.DISCOUNT_FACTOR <= 1, "Gamma debe estar entre 0 y 1"
    assert 0 <= QLearningConfig.EPSILON_END < QLearningConfig.EPSILON_START <= 1, "Epsilon inv√°lido"
    
    # Validar rutas √≥ptimas
    for i, path in enumerate(OptimalPaths.PATHS_4X4):
        assert len(path) == 7, f"Ruta {i+1} debe tener 7 estados"
        assert path[0] == 0 and path[-1] == 15, f"Ruta {i+1} debe ir de 0 a 15"
        assert all(0 <= state <= 15 for state in path), f"Ruta {i+1} contiene estados inv√°lidos"
    
    print(f"{LogConfig.EMOJIS['SUCCESS']} Configuraci√≥n validada correctamente")

if __name__ == "__main__":
    print_config()
    validate_config()
    
    print(f"\n{LogConfig.EMOJIS['OPTIMAL']} Rutas √≥ptimas objetivo:")
    for i, path in enumerate(OptimalPaths.PATHS_4X4, 1):
        path_str = " ‚Üí ".join(map(str, path))
        print(f"  {i}. {path_str}")
    
    print(f"\n{LogConfig.EMOJIS['INFO']} Ventajas de Q-Table vs DQN:")
    print("  ‚Ä¢ Convergencia garantizada en espacios peque√±os")
    print("  ‚Ä¢ Sin problemas de generalizaci√≥n")
    print("  ‚Ä¢ Interpretabilidad completa")
    print("  ‚Ä¢ Memoria perfecta de todas las experiencias")
    print("  ‚Ä¢ No hay overfitting")